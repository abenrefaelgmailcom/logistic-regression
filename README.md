# logistic-regression
logistic regression


פירוט בעברית: מה עשינו בכל שלב ולמה
Exercise 1 — תכונה אחת (דקות פעילות ↔ האם הגיעו ל־10,000 צעדים)

ייבוא ספריות

numpy לעבודה עם מערכים.

matplotlib לציור גרפים.

LogisticRegression כדי לאמן מודל סיווג בינארי.

train_test_split, confusion_matrix, classification_report לשלב ההערכה (בונוס).

בניית מערך הנתונים

X הוא וקטור של מספר דקות פעילות (תכונה אחת), מעוצב ל־2D עם .reshape(-1,1) כי sklearn מצפה למטריצה.

y הוא היעד הבינארי: 1 = הגיעו ל־10K צעדים, 0 = לא.

אימון המודל

solver='liblinear' מתאים למערכי נתונים קטנים־בינוניים ונותן יציבות.

fit(X, y) מעריך את הפרמטרים 
𝛽
0
β
0
	​

 (אינטרספט) ו־
𝛽
1
β
1
	​

 (מקדם של x) ע"י מקסימום סבירות ללוג־לוס (רגרסיה לוגיסטית).

הסקת נוסחת ההסתברות

ההסתברות למעבר/הגעה ל־10K:

𝑃
(
𝑦
=
1
∣
𝑥
)
=
𝜎
(
𝛽
0
+
𝛽
1
𝑥
)
=
1
1
+
𝑒
−
(
𝛽
0
+
𝛽
1
𝑥
)
P(y=1∣x)=σ(β
0
	​

+β
1
	​

x)=
1+e
−(β
0
	​

+β
1
	​

x)
1
	​


זה הופך קו ליניארי ב־x להסתברות בתחום [0,1] באמצעות פונקציית סיגמואיד.

חישוב “Decision Boundary” ל־70%

פותרים 
0.70
=
𝜎
(
𝛽
0
+
𝛽
1
𝑥
)
0.70=σ(β
0
	​

+β
1
	​

x):

log
⁡
0.70
0.30
=
𝛽
0
+
𝛽
1
𝑥
  
  
⇒
  
  
𝑥
=
ln
⁡
(
0.7
/
0.3
)
−
𝛽
0
𝛽
1
log
0.30
0.70
	​

=β
0
	​

+β
1
	​

x⇒x=
β
1
	​

ln(0.7/0.3)−β
0
	​

	​


זה נותן את ערך ה־x שבו המודל מייחס הסתברות 70% ל־y=1.

ציור (Plot)

נקודות הנתונים: פיזור של 
(
𝑥
,
𝑦
)
(x,y) עם y=0/1.

עקומת הסיגמואיד: מחשבים 
𝑃
(
𝑦
=
1
∣
𝑥
)
P(y=1∣x) על טווח צפוף של x ומשרטטים קו חלק.

קווי עזר: קו אופקי ב־p=0.7 וקו אנכי ב־x שבו p=0.7; כיתוב של הנוסחה בגרף.

למה זה חשוב? לראות את ההתאמה של המודל לנתונים ואת הסף שבחרנו.

הערכת מודל (בונוס)

train_test_split כדי להפריד אימון/בדיקה ולמנוע הטיה של הערכה.

confusion_matrix + classification_report כדי לראות טעויות (FP/FN) ומדדים (דיוק, Precision, Recall, F1).

למה? כדי להבין לא רק “כמה” צדקנו (Accuracy) אלא גם איזה סוג טעויות עשה המודל, בהתאם לעלויות/סיכונים בפועל.

Exercise 2 — שתי תכונות (שעות לימוד + שעות שינה ↔ מעבר מבחן)

בניית הנתונים

X2 מטריצה עם שתי תכונות לכל דגימה: [x1, x2] = [שעות לימוד בשבוע, שעות שינה בלילה].

y2 יעד בינארי: 1 = עבר, 0 = נכשל.

אימון מודל לוגיסטי רב־תכונות

שוב solver='liblinear'.

נקבל 
𝛽
0
β
0
	​

 (אינטרספט) ושני מקדמים: 
𝛽
1
β
1
	​

 עבור x1, ו־
𝛽
2
β
2
	​

 עבור x2.

פירוש: שינוי של שעה אחת בלימוד משנה את לוג־האודס במידה 
𝛽
1
β
1
	​

; שינוי של שעה אחת בשינה משנה בלוג־האודס במידה 
𝛽
2
β
2
	​

.

נוסחת ההסתברות

𝑃
(
𝑦
=
1
∣
𝑥
1
,
𝑥
2
)
=
1
1
+
𝑒
−
(
𝛽
0
+
𝛽
1
𝑥
1
+
𝛽
2
𝑥
2
)
P(y=1∣x
1
	​

,x
2
	​

)=
1+e
−(β
0
	​

+β
1
	​

x
1
	​

+β
2
	​

x
2
	​

)
1
	​


מאפשרת לראות כיצד שני המשתנים יחד משפיעים על ההסתברות לעבור.

ציור במישור (x1,x2)

יצרנו “רשת” (meshgrid) על טווחי x1 ו־x2, חישבנו לכל נקודה את 
𝑃
(
𝑦
=
1
)
P(y=1), וציירנו מפה חלקה (contourf) לגווני הסתברות.

גבול החלטה p=0.5: פתרון המשוואה 
𝛽
0
+
𝛽
1
𝑥
1
+
𝛽
2
𝑥
2
=
0
β
0
	​

+β
1
	​

x
1
	​

+β
2
	​

x
2
	​

=0 נותן ישר:

𝑥
2
=
−
𝛽
0
+
𝛽
1
𝑥
1
𝛽
2
x
2
	​

=−
β
2
	​

β
0
	​

+β
1
	​

x
1
	​

	​


זה הקו שבו המודל “אינדיפרנטי” בין 0 ל־1 (סף 0.5).

(אופציונלי) גם קו 
𝑝
=
0.7
p=0.7 כדי לראות אזור הסתברות גבוהה יותר (באמצעות הזזת הלוגיט: 
log
⁡
0.7
0.3
log
0.3
0.7
	​

).

סימון נקודת השאילתה [6.5, 7.5] וחישוב ההסתברות שלה – כדי להבין איפה היא ממוקמת יחסית לגבול.

למה ציור כזה חשוב?

במימד 2, גרף כזה מאפשר להבין איך כל שילוב של לימוד/שינה מתורגם להסתברות, לראות הפרדה בין כיתות (Pass/Fail) ולזהות אזורי אי־ודאות ליד הגבול.

נקודות מתודולוגיות חשובות

סיגמואיד מבטיחה הסתברות ב־[0,1].

מקדמים (β) מפורשים על ציר log-odds (לוגית): תוספת של 1 ביחידה מגדילה/מקטינה את האודס פי 
𝑒
𝛽
e
β
.

סף החלטה (0.5 כברירת מחדל) אפשר להזיז לפי רגישות/ספציפיות רצויות (למשל להשתמש ב־0.7 אם “חיובי שגוי” יקר לנו).

הערכה: לא להסתפק ב־Accuracy — לבדוק Confusion Matrix, Precision, Recall, F1, ורצוי גם ROC-AUC (אם תרצה, אוסיף).
